{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec11cba5-ef80-4640-81e7-596330418162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PGC 2025 Inference\n",
    "\n",
    "Run inference on production CSV files using multiple models:\n",
    "- WeightedCox, RankCox, FullAll, MSE, CE (Transformer-based)\n",
    "- LGBM (LightGBM)\n",
    "- Rule-based (squad_total_health, dist_from_whitezone_v2, dist_from_bluezone_v2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6009aac-3c3c-4cff-8e84-e15192e4b0a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68905e8a-16c8-4cf8-b80a-8b37fc7c35f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install tqdm lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17af9ee5-adcd-4c6d-bdc7-a2756fe34b70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import ast\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightgbm as lgb\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add deployment_v2_1 to path\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "DEPLOYMENT_PATH = os.path.join(PROJECT_ROOT, \"deployment_v2_1\")\n",
    "sys.path.insert(0, DEPLOYMENT_PATH)\n",
    "\n",
    "from deployment_v2_1.src.data.continuous_features import CONTINUOUS_FEATURES\n",
    "from deployment_v2_1.src.data.dataset import PGCDataset\n",
    "from src.models import TransformerBackbone\n",
    "from src.models.heads import get_head\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Deployment path: {DEPLOYMENT_PATH}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f20bcb-b671-46a5-9aac-d98c23691a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = \"/Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_features/prod/\"\n",
    "CHECKPOINT_DIR = \"/Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_results/checkpoints/\"\n",
    "LGBM_MODEL_PATH = \"/Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_results/lgbm/leaves31_lr0.1/model.txt\"\n",
    "OUTPUT_DIR = \"/Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_results/pgc2025_predictions/\"\n",
    "\n",
    "# Device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Constants\n",
    "POSITION_SCALE = 80000.0\n",
    "ZONE_SCALE = 800000.0\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(f\"LGBM model: {LGBM_MODEL_PATH}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c87a9db-ab38-481e-94b4-a141984886f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV files list\n",
    "csv_files = sorted(glob.glob(os.path.join(DATA_PATH, \"*.csv\")))\n",
    "print(f\"Found {len(csv_files)} CSV files\")\n",
    "\n",
    "# Preview first few files\n",
    "for i, f in enumerate(csv_files[:5]):\n",
    "    print(f\"  {i+1}. {os.path.basename(f)}\")\n",
    "if len(csv_files) > 5:\n",
    "    print(f\"  ... and {len(csv_files) - 5} more\")\n",
    "\n",
    "# Show columns of the first CSV file\n",
    "if csv_files:\n",
    "    import pandas as pd\n",
    "    df_sample = pd.read_csv(csv_files[0], nrows=1)\n",
    "    print(f\"\\nColumns in {os.path.basename(csv_files[0])}:\")\n",
    "    print(list(df_sample.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03cc40b8-845b-4d9d-9616-4e2a23fac5e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(set(CONTINUOUS_FEATURES) & set(list(df_sample.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04113ec-8754-410b-a066-c9920a06289f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc55ddf-0024-4414-a836-4a76a55494cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parsing functions\n",
    "_ZONE_FLOAT_RE = re.compile(r'[-+]?(?:\\d*\\.\\d+|\\d+\\.?)(?:[eE][-+]?\\d+)?')\n",
    "\n",
    "\n",
    "def parse_zone_info(zone_str: str) -> Tuple[float, float, float]:\n",
    "    \"\"\"Parse bluezone_info or whitezone_info string to (x, y, radius) tuple.\"\"\"\n",
    "    if pd.isna(zone_str) or zone_str == '':\n",
    "        return 400000.0, 400000.0, 500000.0\n",
    "    \n",
    "    if isinstance(zone_str, (tuple, list)):\n",
    "        if len(zone_str) >= 3:\n",
    "            return float(zone_str[0]), float(zone_str[1]), float(zone_str[2])\n",
    "        return 400000.0, 400000.0, 500000.0\n",
    "    \n",
    "    if isinstance(zone_str, str):\n",
    "        cleaned = re.sub(r'np\\.float\\d+\\s*\\(\\s*([^)]+)\\s*\\)', r'\\1', zone_str)\n",
    "        nums = _ZONE_FLOAT_RE.findall(cleaned)\n",
    "        if len(nums) >= 3:\n",
    "            return float(nums[0]), float(nums[1]), float(nums[2])\n",
    "    \n",
    "    return 400000.0, 400000.0, 500000.0\n",
    "\n",
    "\n",
    "def parse_positions(positions_str: str) -> List[List[float]]:\n",
    "    \"\"\"Parse positions string to list of [x, y, z] coordinates.\"\"\"\n",
    "    if pd.isna(positions_str) or positions_str == '':\n",
    "        return [[np.nan, np.nan, np.nan]]\n",
    "    \n",
    "    positions_str_clean = positions_str.replace('nan', 'None')\n",
    "    positions_list = ast.literal_eval(positions_str_clean)\n",
    "    \n",
    "    result = []\n",
    "    for pos in positions_list:\n",
    "        if pos is None or any(p is None for p in pos):\n",
    "            result.append([np.nan, np.nan, np.nan])\n",
    "        elif pos[0] == 0 and pos[1] == 0 and pos[2] == 0:\n",
    "            result.append([np.nan, np.nan, np.nan])\n",
    "        else:\n",
    "            result.append([float(pos[0]), float(pos[1]), float(pos[2])])\n",
    "    \n",
    "    return result if result else [[np.nan, np.nan, np.nan]]\n",
    "\n",
    "\n",
    "def get_squad_center(positions: List[List[float]]) -> np.ndarray:\n",
    "    \"\"\"Get center position of alive squad members.\"\"\"\n",
    "    positions_arr = np.array(positions)\n",
    "    alive_mask = ~np.isnan(positions_arr[:, 0])\n",
    "    if alive_mask.sum() > 0:\n",
    "        return np.nanmean(positions_arr[alive_mask], axis=0)\n",
    "    return np.array([np.nan, np.nan, np.nan])\n",
    "\n",
    "\n",
    "def compute_dist_from_zone_v2(positions_str: str, zone_str: str) -> float:\n",
    "    \"\"\"Compute distance from squad center to zone center / radius.\"\"\"\n",
    "    positions = parse_positions(positions_str)\n",
    "    zone = parse_zone_info(zone_str)\n",
    "    center = get_squad_center(positions)\n",
    "    \n",
    "    if np.isnan(center[0]) or zone[2] <= 0:\n",
    "        return np.nan\n",
    "    \n",
    "    dist = np.sqrt((center[0] - zone[0])**2 + (center[1] - zone[1])**2)\n",
    "    return dist / zone[2]\n",
    "\n",
    "\n",
    "print(\"Utility functions defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d0ffcfe-6415-4836-a763-7ae877324815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Model Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec3ab971-b62a-4643-a466-dd7ee3851425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_transformer_model(checkpoint_path: str, device: str = \"cpu\") -> Tuple[nn.Module, nn.Module, Dict, Dict, float]:\n",
    "    \"\"\"Load Transformer model from checkpoint.\"\"\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    config_path = os.path.join(os.path.dirname(checkpoint_path), \"config.json\")\n",
    "    \n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "        \n",
    "    input_dim = config.get(\"input_dim\", len(CONTINUOUS_FEATURES))\n",
    "    print(1)\n",
    "    \n",
    "    # Create models\n",
    "    backbone = TransformerBackbone(\n",
    "        input_dim=input_dim,\n",
    "        embed_dim=config[\"embed_dim\"],\n",
    "        num_heads=config[\"num_heads\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        dropout=config[\"dropout\"],\n",
    "    )\n",
    "    print(2)\n",
    "    \n",
    "    loss_type = config.get(\"loss_type\", \"weighted_cox\")\n",
    "    head = get_head(loss_type, embed_dim=config[\"embed_dim\"], dropout=config[\"dropout\"])\n",
    "    print(3)\n",
    "\n",
    "    # Load weights\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    backbone.load_state_dict(checkpoint[\"backbone_state_dict\"])\n",
    "    head.load_state_dict(checkpoint[\"head_state_dict\"])\n",
    "    scaler_params = checkpoint.get(\"scaler_params\", None)\n",
    "    print(4)\n",
    "    backbone = backbone.to(device).eval()\n",
    "    head = head.to(device).eval()\n",
    "    print(5)\n",
    "    # Load temperature\n",
    "    tau_path = os.path.join(checkpoint_dir, \"temperature.json\")\n",
    "    temperature = 1.0\n",
    "    if os.path.exists(tau_path):\n",
    "        with open(tau_path, \"r\") as f:\n",
    "            tau_data = json.load(f)\n",
    "        temperature = tau_data.get(\"optimal_tau\", 1.0)\n",
    "    \n",
    "    return backbone, head, scaler_params, config, temperature\n",
    "\n",
    "\n",
    "def load_lgbm_model(model_path: str) -> Tuple[lgb.Booster, Dict]:\n",
    "    \"\"\"Load LightGBM model and its config.\"\"\"\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "    \n",
    "    config_path = os.path.join(os.path.dirname(model_path), \"config.json\")\n",
    "    config = {}\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "    \n",
    "    return model, config\n",
    "\n",
    "\n",
    "print(\"Model loading functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aedab05-91b4-4678-a75b-a15b7f0e4e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specific checkpoint subdirectory names for each model\n",
    "CHECKPOINT_PATHS = {\n",
    "    # \"MSE\": \"emb256_head8_layer4_drop0.1_lr1e-4_mse\",\n",
    "    # \"FullAll\": \"run21_emb1024_head8_layer6_drop0.1_lr1e-4_cox\",\n",
    "    # \"RankCox\": \"run22_emb512_head4_layer6_drop0.1_lr1e-4_rank_cox\",\n",
    "    \"WeightedCox\": \"final_model\",\n",
    "    # \"CE\": \"run25_emb512_head4_layer6_drop0.1_lr1e-4_survival_ce\",\n",
    "}\n",
    "\n",
    "# Find checkpoints for each model\n",
    "model_checkpoints = {}\n",
    "for model_name, subdir in CHECKPOINT_PATHS.items():\n",
    "    best_path = os.path.join(CHECKPOINT_DIR, subdir, \"best.pt\")\n",
    "    if os.path.exists(best_path):\n",
    "        model_checkpoints[model_name] = best_path\n",
    "        print(f\"{model_name}: {subdir}\")\n",
    "    else:\n",
    "        print(f\"{model_name}: NOT FOUND ({best_path})\")\n",
    "\n",
    "print(f\"\\nFound {len(model_checkpoints)} Transformer model checkpoints\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00835e66-726b-464d-be85-5e287e62113c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Inference Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c055223c-ed85-4050-8f25-25b07772dd44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_transformer_inference(\n",
    "    csv_path: str,\n",
    "    backbone: nn.Module,\n",
    "    head: nn.Module,\n",
    "    scaler_params: Dict,\n",
    "    config: Dict,\n",
    "    device: str = \"cpu\",\n",
    "    temperature: float = 1.0,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Run Transformer model inference on a single CSV file.\"\"\"\n",
    "    use_dataset_v2 = config.get(\"use_dataset_v2\", True)\n",
    "    \n",
    "    csv_path = os.path.abspath(csv_path)\n",
    "    folder_path = os.path.dirname(csv_path)\n",
    "    file_name = os.path.basename(csv_path)\n",
    "    \n",
    "    if use_dataset_v2:\n",
    "        dataset = PGCDataset(\n",
    "            folder_path=folder_path,\n",
    "            file_list=[file_name],\n",
    "            continuous_features=CONTINUOUS_FEATURES,\n",
    "            scaler_params=scaler_params,\n",
    "        )\n",
    "    else:\n",
    "        dataset = PGCDataset(\n",
    "            folder_path=folder_path,\n",
    "            file_list=[file_name],\n",
    "            continuous_features=CONTINUOUS_FEATURES,\n",
    "            scaler_params=scaler_params,\n",
    "        )\n",
    "    \n",
    "    raw_df = pd.read_csv(csv_path)\n",
    "    raw_df['time_point_key'] = raw_df['time_point'].round(6)\n",
    "    time_point_groups = {}\n",
    "    for (mid, tp_key), group in raw_df.groupby(['match_id', 'time_point_key']):\n",
    "        time_point_groups[(mid, tp_key)] = group.sort_values('squad_number')\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(dataset)):\n",
    "            match_id, time_point = dataset.group_keys[idx]\n",
    "            sample = dataset[idx]\n",
    "            \n",
    "            x = sample[\"x_continuous\"].unsqueeze(0).to(device)\n",
    "            positions = sample[\"positions\"].unsqueeze(0).to(device)\n",
    "            bluezone = sample[\"bluezone_info\"].unsqueeze(0).to(device)\n",
    "            whitezone = sample[\"whitezone_info\"].unsqueeze(0).to(device)\n",
    "            map_idx = sample[\"map_idx\"].unsqueeze(0).to(device)\n",
    "            num_squads = sample[\"num_squads\"]\n",
    "            \n",
    "            time_point_key = round(time_point, 6)\n",
    "            time_data = time_point_groups.get((match_id, time_point_key))\n",
    "            if time_data is None:\n",
    "                continue\n",
    "            \n",
    "            time_data = time_data.head(num_squads)\n",
    "            squad_numbers = time_data['squad_number'].tolist()\n",
    "            \n",
    "            alive_cnt = [int(x) for x in time_data['squad_alive_count'].tolist()] if 'squad_alive_count' in time_data.columns else [None] * len(squad_numbers)\n",
    "            hp = [int(x) for x in time_data['squad_total_health'].tolist()] if 'squad_total_health' in time_data.columns else [None] * len(squad_numbers)\n",
    "            \n",
    "            positions_list = []\n",
    "            if 'positions' in time_data.columns:\n",
    "                for pos_str in time_data['positions'].tolist():\n",
    "                    positions_list.append(parse_positions(pos_str))\n",
    "            \n",
    "            # Build alive mask\n",
    "            squad_alive_mask = torch.zeros(num_squads, dtype=torch.bool)\n",
    "            for i in range(len(squad_numbers)):\n",
    "                alive_count = alive_cnt[i] if alive_cnt[i] is not None else 0\n",
    "                health = hp[i] if hp[i] is not None else 0\n",
    "                all_positions_nan = True\n",
    "                if i < len(positions_list):\n",
    "                    for pos in positions_list[i]:\n",
    "                        if not (np.isnan(pos[0]) and np.isnan(pos[1]) and np.isnan(pos[2])):\n",
    "                            all_positions_nan = False\n",
    "                            break\n",
    "                squad_alive_mask[i] = (alive_count > 0) and (health > 0) and (not all_positions_nan)\n",
    "            \n",
    "            # Forward pass\n",
    "            embeddings = backbone(x, positions, bluezone, whitezone, map_idx)\n",
    "            raw_scores = head(embeddings).squeeze(0).cpu()\n",
    "            scaled_scores = raw_scores / temperature\n",
    "            \n",
    "            masked_scores = scaled_scores.clone()\n",
    "            masked_scores[~squad_alive_mask] = float(\"-inf\")\n",
    "            win_probs = torch.softmax(masked_scores[:num_squads], dim=0)\n",
    "            \n",
    "            # Parse zone info\n",
    "            bluezone_parsed = parse_zone_info(time_data['bluezone_info'].iloc[0]) if 'bluezone_info' in time_data.columns else None\n",
    "            whitezone_parsed = parse_zone_info(time_data['whitezone_info'].iloc[0]) if 'whitezone_info' in time_data.columns else None\n",
    "            phase = int(time_data['phase'].iloc[0]) if 'phase' in time_data.columns else None\n",
    "            \n",
    "            # Build probabilities dict\n",
    "            probabilities = {}\n",
    "            is_alive = {}\n",
    "            for i, squad_num in enumerate(squad_numbers):\n",
    "                if i < len(win_probs):\n",
    "                    probabilities[int(squad_num)] = float(win_probs[i].item()) if squad_alive_mask[i] else 0.0\n",
    "                is_alive[int(squad_num)] = bool(squad_alive_mask[i])\n",
    "            \n",
    "            predictions[str(time_point)] = {\n",
    "                'phase': phase,\n",
    "                'squad_numbers': [int(sn) for sn in squad_numbers],\n",
    "                'alive_cnt': alive_cnt,\n",
    "                'hp': hp,\n",
    "                'bluezone_info': {'x': bluezone_parsed[0], 'y': bluezone_parsed[1], 'radius': bluezone_parsed[2]} if bluezone_parsed else None,\n",
    "                'whitezone_info': {'x': whitezone_parsed[0], 'y': whitezone_parsed[1], 'radius': whitezone_parsed[2]} if whitezone_parsed else None,\n",
    "                'positions': positions_list,\n",
    "                'probabilities': probabilities,\n",
    "                'is_alive': is_alive,\n",
    "            }\n",
    "    \n",
    "    return {'match_id': match_id, 'predictions': predictions}\n",
    "\n",
    "\n",
    "print(\"Transformer inference function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d63ca4-68cf-4b45-b441-b8b8324c9b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_lgbm_features(df: pd.DataFrame, scaler_params: Dict, all_features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Extract and scale features for LGBM model.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Position features\n",
    "    pos_features = ['team_center_x', 'team_center_y', 'team_center_z', \n",
    "                    'team_std_x', 'team_std_y', 'team_std_z',\n",
    "                    'bluezone_x', 'bluezone_y', 'whitezone_x', 'whitezone_y']\n",
    "    for col in pos_features:\n",
    "        df[col] = 0.0\n",
    "    \n",
    "    # Distance features\n",
    "    df['dist_from_bluezone_v2'] = 0.0\n",
    "    df['dist_from_whitezone_v2'] = 0.0\n",
    "    \n",
    "    for idx in df.index:\n",
    "        row = df.loc[idx]\n",
    "        team_center_xy = None\n",
    "        \n",
    "        # Parse positions\n",
    "        try:\n",
    "            positions = np.array(parse_positions(row['positions']))\n",
    "            alive_mask = ~np.isnan(positions[:, 0])\n",
    "            alive_positions = positions[alive_mask]\n",
    "            \n",
    "            if len(alive_positions) > 0:\n",
    "                center = np.nanmean(alive_positions, axis=0)\n",
    "                team_center_xy = center[:2]\n",
    "                \n",
    "                df.loc[idx, 'team_center_x'] = center[0] / POSITION_SCALE\n",
    "                df.loc[idx, 'team_center_y'] = center[1] / POSITION_SCALE\n",
    "                df.loc[idx, 'team_center_z'] = center[2] / POSITION_SCALE\n",
    "                \n",
    "                if len(alive_positions) > 1:\n",
    "                    std = np.nanstd(alive_positions, axis=0) / POSITION_SCALE\n",
    "                    df.loc[idx, 'team_std_x'] = std[0]\n",
    "                    df.loc[idx, 'team_std_y'] = std[1]\n",
    "                    df.loc[idx, 'team_std_z'] = std[2]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Parse zone info\n",
    "        bluezone, whitezone = None, None\n",
    "        try:\n",
    "            bluezone = parse_zone_info(row['bluezone_info'])\n",
    "            df.loc[idx, 'bluezone_x'] = bluezone[0] / ZONE_SCALE\n",
    "            df.loc[idx, 'bluezone_y'] = bluezone[1] / ZONE_SCALE\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            whitezone = parse_zone_info(row['whitezone_info'])\n",
    "            df.loc[idx, 'whitezone_x'] = whitezone[0] / ZONE_SCALE\n",
    "            df.loc[idx, 'whitezone_y'] = whitezone[1] / ZONE_SCALE\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Compute distance features\n",
    "        if team_center_xy is not None:\n",
    "            if bluezone is not None and bluezone[2] > 0:\n",
    "                dist_blue = np.sqrt((team_center_xy[0] - bluezone[0])**2 + (team_center_xy[1] - bluezone[1])**2)\n",
    "                df.loc[idx, 'dist_from_bluezone_v2'] = dist_blue / bluezone[2]\n",
    "            \n",
    "            if whitezone is not None and whitezone[2] > 0:\n",
    "                dist_white = np.sqrt((team_center_xy[0] - whitezone[0])**2 + (team_center_xy[1] - whitezone[1])**2)\n",
    "                df.loc[idx, 'dist_from_whitezone_v2'] = dist_white / whitezone[2]\n",
    "    \n",
    "    # Apply scaling to continuous features\n",
    "    if scaler_params:\n",
    "        mean = scaler_params.get('mean', {})\n",
    "        std = scaler_params.get('std', {})\n",
    "        for col in CONTINUOUS_FEATURES:\n",
    "            if col in df.columns and col in mean and col in std:\n",
    "                df[col] = (df[col] - mean[col]) / std[col]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def run_lgbm_inference(csv_path: str, model: lgb.Booster, config: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"Run LGBM model inference on a single CSV file.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    match_id = df['match_id'].iloc[0]\n",
    "    \n",
    "    all_features = config.get('features', CONTINUOUS_FEATURES)\n",
    "    scaler_params = None  # LGBM handles scaling internally during training\n",
    "    \n",
    "    df = extract_lgbm_features(df, scaler_params, all_features)\n",
    "    \n",
    "    # Predict survival time (higher = longer survival)\n",
    "    features_available = [f for f in all_features if f in df.columns]\n",
    "    df['pred'] = model.predict(df[features_available])\n",
    "    \n",
    "    predictions = {}\n",
    "    for time_point, tp_df in df.groupby('time_point'):\n",
    "        tp_df = tp_df.sort_values('squad_number')\n",
    "        squad_numbers = tp_df['squad_number'].tolist()\n",
    "        \n",
    "        alive_cnt = [int(x) for x in tp_df['squad_alive_count'].tolist()] if 'squad_alive_count' in tp_df.columns else [None] * len(squad_numbers)\n",
    "        hp = [int(x) for x in tp_df['squad_total_health'].tolist()] if 'squad_total_health' in tp_df.columns else [None] * len(squad_numbers)\n",
    "        \n",
    "        positions_list = []\n",
    "        if 'positions' in tp_df.columns:\n",
    "            for pos_str in tp_df['positions'].tolist():\n",
    "                positions_list.append(parse_positions(pos_str))\n",
    "        \n",
    "        # Build alive mask\n",
    "        squad_alive_mask = []\n",
    "        for i in range(len(squad_numbers)):\n",
    "            alive_count = alive_cnt[i] if alive_cnt[i] is not None else 0\n",
    "            health = hp[i] if hp[i] is not None else 0\n",
    "            all_positions_nan = True\n",
    "            if i < len(positions_list):\n",
    "                for pos in positions_list[i]:\n",
    "                    if not (np.isnan(pos[0]) and np.isnan(pos[1]) and np.isnan(pos[2])):\n",
    "                        all_positions_nan = False\n",
    "                        break\n",
    "            squad_alive_mask.append((alive_count > 0) and (health > 0) and (not all_positions_nan))\n",
    "        \n",
    "        # Compute probabilities via softmax (higher pred = higher prob)\n",
    "        preds = tp_df['pred'].values\n",
    "        masked_preds = np.where(squad_alive_mask, preds, -np.inf)\n",
    "        probs = softmax(masked_preds)\n",
    "        \n",
    "        probabilities = {}\n",
    "        is_alive = {}\n",
    "        for i, squad_num in enumerate(squad_numbers):\n",
    "            probabilities[int(squad_num)] = float(probs[i]) if squad_alive_mask[i] else 0.0\n",
    "            is_alive[int(squad_num)] = squad_alive_mask[i]\n",
    "        \n",
    "        bluezone_parsed = parse_zone_info(tp_df['bluezone_info'].iloc[0]) if 'bluezone_info' in tp_df.columns else None\n",
    "        whitezone_parsed = parse_zone_info(tp_df['whitezone_info'].iloc[0]) if 'whitezone_info' in tp_df.columns else None\n",
    "        phase = int(tp_df['phase'].iloc[0]) if 'phase' in tp_df.columns else None\n",
    "        \n",
    "        predictions[str(time_point)] = {\n",
    "            'phase': phase,\n",
    "            'squad_numbers': [int(sn) for sn in squad_numbers],\n",
    "            'alive_cnt': alive_cnt,\n",
    "            'hp': hp,\n",
    "            'bluezone_info': {'x': bluezone_parsed[0], 'y': bluezone_parsed[1], 'radius': bluezone_parsed[2]} if bluezone_parsed else None,\n",
    "            'whitezone_info': {'x': whitezone_parsed[0], 'y': whitezone_parsed[1], 'radius': whitezone_parsed[2]} if whitezone_parsed else None,\n",
    "            'positions': positions_list,\n",
    "            'probabilities': probabilities,\n",
    "            'is_alive': is_alive,\n",
    "        }\n",
    "    \n",
    "    return {'match_id': match_id, 'predictions': predictions}\n",
    "\n",
    "\n",
    "print(\"LGBM inference function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74edda39-ee52-4f9b-b1b7-580644c9102e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_rule_based_inference(csv_path: str, feature_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run rule-based inference using a single feature.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to CSV file.\n",
    "        feature_name: Feature to use for prediction.\n",
    "            - 'squad_total_health': higher is better\n",
    "            - 'dist_from_whitezone_v2': lower is better (closer to next safe zone)\n",
    "            - 'dist_from_bluezone_v2': lower is better (closer to current safe zone)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    match_id = df['match_id'].iloc[0]\n",
    "    \n",
    "    # Compute distance features if needed\n",
    "    if feature_name in ['dist_from_whitezone_v2', 'dist_from_bluezone_v2']:\n",
    "        for idx in df.index:\n",
    "            row = df.loc[idx]\n",
    "            if feature_name == 'dist_from_bluezone_v2':\n",
    "                df.loc[idx, 'dist_from_bluezone_v2'] = compute_dist_from_zone_v2(\n",
    "                    row['positions'], row['bluezone_info']\n",
    "                )\n",
    "            elif feature_name == 'dist_from_whitezone_v2':\n",
    "                df.loc[idx, 'dist_from_whitezone_v2'] = compute_dist_from_zone_v2(\n",
    "                    row['positions'], row['whitezone_info']\n",
    "                )\n",
    "    \n",
    "    predictions = {}\n",
    "    for time_point, tp_df in df.groupby('time_point'):\n",
    "        tp_df = tp_df.sort_values('squad_number')\n",
    "        squad_numbers = tp_df['squad_number'].tolist()\n",
    "        \n",
    "        alive_cnt = [int(x) for x in tp_df['squad_alive_count'].tolist()] if 'squad_alive_count' in tp_df.columns else [None] * len(squad_numbers)\n",
    "        hp = [int(x) for x in tp_df['squad_total_health'].tolist()] if 'squad_total_health' in tp_df.columns else [None] * len(squad_numbers)\n",
    "        \n",
    "        positions_list = []\n",
    "        if 'positions' in tp_df.columns:\n",
    "            for pos_str in tp_df['positions'].tolist():\n",
    "                positions_list.append(parse_positions(pos_str))\n",
    "        \n",
    "        # Build alive mask\n",
    "        squad_alive_mask = []\n",
    "        for i in range(len(squad_numbers)):\n",
    "            alive_count = alive_cnt[i] if alive_cnt[i] is not None else 0\n",
    "            health = hp[i] if hp[i] is not None else 0\n",
    "            all_positions_nan = True\n",
    "            if i < len(positions_list):\n",
    "                for pos in positions_list[i]:\n",
    "                    if not (np.isnan(pos[0]) and np.isnan(pos[1]) and np.isnan(pos[2])):\n",
    "                        all_positions_nan = False\n",
    "                        break\n",
    "            squad_alive_mask.append((alive_count > 0) and (health > 0) and (not all_positions_nan))\n",
    "        \n",
    "        # Get feature values and apply rule\n",
    "        feature_values = tp_df[feature_name].values.copy()\n",
    "        \n",
    "        # For distance features, lower is better (use negative)\n",
    "        # For squad_total_health, higher is better (keep as is)\n",
    "        if feature_name in ['dist_from_whitezone_v2', 'dist_from_bluezone_v2']:\n",
    "            feature_values = -feature_values  # Negate so higher = better\n",
    "        \n",
    "        # Handle NaN values\n",
    "        feature_values = np.nan_to_num(feature_values, nan=-np.inf)\n",
    "        \n",
    "        # Mask dead squads\n",
    "        masked_values = np.where(squad_alive_mask, feature_values, -np.inf)\n",
    "        probs = softmax(masked_values)\n",
    "        \n",
    "        probabilities = {}\n",
    "        is_alive = {}\n",
    "        for i, squad_num in enumerate(squad_numbers):\n",
    "            probabilities[int(squad_num)] = float(probs[i]) if squad_alive_mask[i] else 0.0\n",
    "            is_alive[int(squad_num)] = squad_alive_mask[i]\n",
    "        \n",
    "        bluezone_parsed = parse_zone_info(tp_df['bluezone_info'].iloc[0]) if 'bluezone_info' in tp_df.columns else None\n",
    "        whitezone_parsed = parse_zone_info(tp_df['whitezone_info'].iloc[0]) if 'whitezone_info' in tp_df.columns else None\n",
    "        phase = int(tp_df['phase'].iloc[0]) if 'phase' in tp_df.columns else None\n",
    "        \n",
    "        predictions[str(time_point)] = {\n",
    "            'phase': phase,\n",
    "            'squad_numbers': [int(sn) for sn in squad_numbers],\n",
    "            'alive_cnt': alive_cnt,\n",
    "            'hp': hp,\n",
    "            'bluezone_info': {'x': bluezone_parsed[0], 'y': bluezone_parsed[1], 'radius': bluezone_parsed[2]} if bluezone_parsed else None,\n",
    "            'whitezone_info': {'x': whitezone_parsed[0], 'y': whitezone_parsed[1], 'radius': whitezone_parsed[2]} if whitezone_parsed else None,\n",
    "            'positions': positions_list,\n",
    "            'probabilities': probabilities,\n",
    "            'is_alive': is_alive,\n",
    "        }\n",
    "    \n",
    "    return {'match_id': match_id, 'predictions': predictions}\n",
    "\n",
    "\n",
    "print(\"Rule-based inference function defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d762118-a84a-439c-ad3f-bd26ddd6fa5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. JSON Saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26069171-9671-4bc7-a111-37b6380a920d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_predictions_to_json(result: Dict[str, Any], output_dir: str, model_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Save predictions to JSON file.\n",
    "    \n",
    "    Args:\n",
    "        result: Dict with 'match_id' and 'predictions' keys.\n",
    "        output_dir: Output directory path.\n",
    "        model_name: Model name for filename.\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved JSON file.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    match_id = result['match_id']\n",
    "    json_filename = f\"{match_id}_{model_name}.json\"\n",
    "    json_path = os.path.join(output_dir, json_filename)\n",
    "    \n",
    "    output_data = {\n",
    "        'match_id': match_id,\n",
    "        'model_name': model_name,\n",
    "        'predictions': result['predictions'],\n",
    "    }\n",
    "    \n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return json_path\n",
    "\n",
    "\n",
    "print(\"JSON saving function defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1edc682d-9353-4f9e-a93d-26757b692879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Main Execution Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dee17f4-f8dd-48c9-8f0e-95832e9aec02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load all Transformer models\n",
    "transformer_models = {}\n",
    "for model_name, checkpoint_path in model_checkpoints.items():\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    backbone, head, scaler_params, config, temperature = load_transformer_model(checkpoint_path, DEVICE)\n",
    "    transformer_models[model_name] = {\n",
    "        'backbone': backbone,\n",
    "        'head': head,\n",
    "        'scaler_params': scaler_params,\n",
    "        'config': config,\n",
    "        'temperature': temperature,\n",
    "    }\n",
    "    print(f\"  Loaded: embed_dim={config['embed_dim']}, temperature={temperature:.4f}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(transformer_models)} Transformer models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e60cf30-c91e-4f63-ad84-234e4d83dde1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load LGBM model\n",
    "lgbm_model, lgbm_config = None, {}\n",
    "if os.path.exists(LGBM_MODEL_PATH):\n",
    "    print(\"Loading LGBM model...\")\n",
    "    lgbm_model, lgbm_config = load_lgbm_model(LGBM_MODEL_PATH)\n",
    "    print(f\"  Features: {len(lgbm_config.get('features', []))}\")\n",
    "else:\n",
    "    print(f\"LGBM model not found at: {LGBM_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6e27077-3bc5-4c4d-9ed9-9859cb480a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_all_inference(csv_files: List[str], output_dir: str):\n",
    "    \"\"\"Run inference with all models on all CSV files.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Define all models to run\n",
    "    all_models = list(transformer_models.keys())\n",
    "    if lgbm_model is not None:\n",
    "        all_models.append('LGBM')\n",
    "    all_models.extend(['RuleBased_HP', 'RuleBased_WhiteZone', 'RuleBased_BlueZone'])\n",
    "    \n",
    "    print(f\"Models to run: {all_models}\")\n",
    "    print(f\"CSV files: {len(csv_files)}\")\n",
    "    print(f\"Output dir: {output_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for csv_path in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "        csv_basename = os.path.basename(csv_path)\n",
    "        \n",
    "        # Transformer models\n",
    "        for model_name, model_data in transformer_models.items():\n",
    "            try:\n",
    "                result = run_transformer_inference(\n",
    "                    csv_path=csv_path,\n",
    "                    backbone=model_data['backbone'],\n",
    "                    head=model_data['head'],\n",
    "                    scaler_params=model_data['scaler_params'],\n",
    "                    config=model_data['config'],\n",
    "                    device=DEVICE,\n",
    "                    temperature=model_data['temperature'],\n",
    "                )\n",
    "                save_predictions_to_json(result, output_dir, model_name)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error {model_name} on {csv_basename}: {e}\")\n",
    "        \n",
    "        # # LGBM model\n",
    "        # if lgbm_model is not None:\n",
    "        #     try:\n",
    "        #         result = run_lgbm_inference(csv_path, lgbm_model, lgbm_config)\n",
    "        #         save_predictions_to_json(result, output_dir, 'LGBM')\n",
    "        #     except Exception as e:\n",
    "        #         print(f\"  Error LGBM on {csv_basename}: {e}\")\n",
    "        \n",
    "        # Rule-based models\n",
    "        rule_based_configs = [\n",
    "            ('RuleBased_HP', 'squad_total_health'),\n",
    "            ('RuleBased_WhiteZone', 'dist_from_whitezone_v2'),\n",
    "            ('RuleBased_BlueZone', 'dist_from_bluezone_v2'),\n",
    "        ]\n",
    "        \n",
    "        for model_name, feature_name in rule_based_configs:\n",
    "            try:\n",
    "                result = run_rule_based_inference(csv_path, feature_name)\n",
    "                save_predictions_to_json(result, output_dir, model_name)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error {model_name} on {csv_basename}: {e}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Inference complete!\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "\n",
    "print(\"Main execution function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a5b170-c6b1-41dc-a7da-647631d7ebb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run inference on all CSV files\n",
    "run_all_inference(csv_files, OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9fc981a-588c-4fd0-9cb1-ed28c6b88993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Verify Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3b2492c-dc13-4f48-b0a9-4bf689b40b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List generated JSON files\n",
    "output_json_files = sorted(glob.glob(os.path.join(OUTPUT_DIR, \"*.json\")))\n",
    "print(f\"Total JSON files generated: {len(output_json_files)}\")\n",
    "\n",
    "# Group by model\n",
    "model_counts = {}\n",
    "for f in output_json_files:\n",
    "    basename = os.path.basename(f)\n",
    "    model_name = basename.rsplit('_', 1)[-1].replace('.json', '')\n",
    "    model_counts[model_name] = model_counts.get(model_name, 0) + 1\n",
    "\n",
    "print(\"\\nFiles per model:\")\n",
    "for model_name, count in sorted(model_counts.items()):\n",
    "    print(f\"  {model_name}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80f549fc-9d54-4c17-83d1-a03acd17b40b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Examine a sample JSON file\n",
    "if output_json_files:\n",
    "    sample_json_path = output_json_files[0]\n",
    "    with open(sample_json_path, 'r') as f:\n",
    "        sample_data = json.load(f)\n",
    "    \n",
    "    print(f\"Sample JSON: {os.path.basename(sample_json_path)}\")\n",
    "    print(f\"Match ID: {sample_data['match_id']}\")\n",
    "    print(f\"Model: {sample_data['model_name']}\")\n",
    "    print(f\"Time points: {len(sample_data['predictions'])}\")\n",
    "    \n",
    "    # Show first time point structure\n",
    "    first_tp = list(sample_data['predictions'].keys())[0]\n",
    "    tp_data = sample_data['predictions'][first_tp]\n",
    "    print(f\"\\nFirst time point ({first_tp}):\")\n",
    "    print(f\"  Phase: {tp_data['phase']}\")\n",
    "    print(f\"  Squads: {len(tp_data['squad_numbers'])}\")\n",
    "    print(f\"  Alive squads: {sum(tp_data['is_alive'].values())}\")\n",
    "    print(f\"  Keys: {list(tp_data.keys())}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pgc2025_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
