{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import glob\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Configuration\n",
        "# ============================================================================\n",
        "\n",
        "PREDICTIONS_DIR = \"/Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_results/pgc2025_predictions/\"\n",
        "DATA_PATH = \"/Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_features/prod/\"\n",
        "\n",
        "NUM_PHASES = 10\n",
        "MODELS = [\"WeightedCox\", \"RuleBased_HP\", \"RuleBased_WhiteZone\", \"RuleBased_BlueZone\"]\n",
        "\n",
        "print(f\"Predictions directory: {PREDICTIONS_DIR}\")\n",
        "print(f\"Ground truth data path: {DATA_PATH}\")\n",
        "print(f\"Models to evaluate: {MODELS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Data Loading Functions\n",
        "# ============================================================================\n",
        "\n",
        "def load_prediction_json(json_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Load a single prediction JSON file.\"\"\"\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_ground_truth_csv(csv_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load ground truth CSV with squad_win column.\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    return df\n",
        "\n",
        "\n",
        "def match_predictions_with_truth(match_id: str, predictions: Dict, ground_truth: pd.DataFrame) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Match predictions with ground truth data.\n",
        "    \n",
        "    Args:\n",
        "        match_id: Match ID string\n",
        "        predictions: Prediction data from JSON (format: {time_point_str: {...}})\n",
        "        ground_truth: Ground truth DataFrame\n",
        "    \n",
        "    Returns:\n",
        "        List of matched records with predictions and ground truth\n",
        "    \"\"\"\n",
        "    matched_data = []\n",
        "    \n",
        "    for time_point_str, pred_data in predictions.items():\n",
        "        time_point = float(time_point_str)\n",
        "        \n",
        "        # Find matching rows in ground truth\n",
        "        gt_rows = ground_truth[\n",
        "            (ground_truth['time_point'].round(6) == round(time_point, 6))\n",
        "        ]\n",
        "        \n",
        "        if len(gt_rows) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Get winner squad number\n",
        "        winner_rows = gt_rows[gt_rows['squad_win'] == 1]\n",
        "        if len(winner_rows) == 0:\n",
        "            continue\n",
        "        \n",
        "        winner_squad = int(winner_rows.iloc[0]['squad_number'])\n",
        "        phase = pred_data.get('phase', None)\n",
        "        probabilities_raw = pred_data.get('probabilities', {})\n",
        "        is_alive_raw = pred_data.get('is_alive', {})\n",
        "        \n",
        "        # CRITICAL FIX: Convert string keys to integers (JSON keys are always strings)\n",
        "        probabilities = {int(k): float(v) for k, v in probabilities_raw.items()}\n",
        "        is_alive = {int(k): bool(v) for k, v in is_alive_raw.items()}\n",
        "        \n",
        "        # CRITICAL FIX: Normalize probabilities to sum to 1.0 (only for alive squads)\n",
        "        alive_probs = {k: v for k, v in probabilities.items() if is_alive.get(k, False)}\n",
        "        total_prob = sum(alive_probs.values())\n",
        "        \n",
        "        if total_prob > 0:\n",
        "            # Renormalize to sum to 1.0\n",
        "            probabilities_normalized = {k: v / total_prob for k, v in alive_probs.items()}\n",
        "        else:\n",
        "            # If all probabilities are 0, use uniform distribution over alive squads\n",
        "            if len(alive_probs) > 0:\n",
        "                uniform_prob = 1.0 / len(alive_probs)\n",
        "                probabilities_normalized = {k: uniform_prob for k in alive_probs.keys()}\n",
        "            else:\n",
        "                probabilities_normalized = {}\n",
        "        \n",
        "        # Count alive squads\n",
        "        num_alive = sum(is_alive.values())\n",
        "        \n",
        "        # Skip if winner is not alive (data inconsistency)\n",
        "        if winner_squad not in probabilities_normalized:\n",
        "            continue\n",
        "        \n",
        "        matched_data.append({\n",
        "            'match_id': match_id,\n",
        "            'time_point': time_point,\n",
        "            'phase': phase,\n",
        "            'winner_squad': winner_squad,\n",
        "            'probabilities': probabilities_normalized,\n",
        "            'is_alive': is_alive,\n",
        "            'num_alive': num_alive,\n",
        "        })\n",
        "    \n",
        "    return matched_data\n",
        "\n",
        "\n",
        "print(\"Data loading functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Metric Calculation Functions\n",
        "# ============================================================================\n",
        "\n",
        "def compute_accuracy(probs_dict: Dict[int, float], winner_squad: int) -> float:\n",
        "    \"\"\"\n",
        "    Compute accuracy: whether predicted winner matches actual winner.\n",
        "    \n",
        "    Args:\n",
        "        probs_dict: Dictionary mapping squad_number (int) to probability (normalized)\n",
        "        winner_squad: Actual winner squad number (int)\n",
        "    \n",
        "    Returns:\n",
        "        1.0 if correct, 0.0 otherwise\n",
        "    \"\"\"\n",
        "    if not probs_dict:\n",
        "        return 0.0\n",
        "    \n",
        "    predicted_winner = max(probs_dict, key=probs_dict.get)\n",
        "    return 1.0 if predicted_winner == winner_squad else 0.0\n",
        "\n",
        "\n",
        "def compute_log_loss(probs_dict: Dict[int, float], winner_squad: int, eps: float = 1e-15) -> float:\n",
        "    \"\"\"\n",
        "    Compute log loss (cross-entropy) for multi-class classification.\n",
        "    \n",
        "    Log Loss = -log(P(y_true))\n",
        "    \n",
        "    Args:\n",
        "        probs_dict: Dictionary mapping squad_number (int) to probability (normalized)\n",
        "        winner_squad: Actual winner squad number (int)\n",
        "        eps: Small value to avoid log(0)\n",
        "    \n",
        "    Returns:\n",
        "        Log loss value\n",
        "    \"\"\"\n",
        "    if winner_squad not in probs_dict:\n",
        "        # Winner not in probabilities (should not happen after filtering)\n",
        "        return float('inf')\n",
        "    \n",
        "    prob = probs_dict[winner_squad]\n",
        "    prob = np.clip(prob, eps, 1.0 - eps)  # Clip to avoid log(0) and log(1)\n",
        "    return -np.log(prob)\n",
        "\n",
        "\n",
        "def compute_brier_score(probs_dict: Dict[int, float], winner_squad: int) -> float:\n",
        "    \"\"\"\n",
        "    Compute Brier score (mean squared error between predicted probabilities and actual outcome).\n",
        "    \n",
        "    Brier Score = sum_i (p_i - y_i)^2\n",
        "    where y_i = 1 if squad_i is winner, 0 otherwise\n",
        "    \n",
        "    Args:\n",
        "        probs_dict: Dictionary mapping squad_number (int) to probability (normalized)\n",
        "        winner_squad: Actual winner squad number (int)\n",
        "    \n",
        "    Returns:\n",
        "        Brier score value\n",
        "    \"\"\"\n",
        "    brier = 0.0\n",
        "    for squad, prob in probs_dict.items():\n",
        "        true_value = 1.0 if squad == winner_squad else 0.0\n",
        "        brier += (prob - true_value) ** 2\n",
        "    \n",
        "    return brier\n",
        "\n",
        "\n",
        "def compute_ece(all_predictions: List[Dict], n_bins: int = 10) -> float:\n",
        "    \"\"\"\n",
        "    Compute Expected Calibration Error (ECE).\n",
        "    \n",
        "    ECE measures how well predicted probabilities match actual accuracy.\n",
        "    \n",
        "    Args:\n",
        "        all_predictions: List of prediction dictionaries\n",
        "        n_bins: Number of bins for calibration\n",
        "    \n",
        "    Returns:\n",
        "        ECE value\n",
        "    \"\"\"\n",
        "    confidences = []\n",
        "    correctness = []\n",
        "    \n",
        "    for pred in all_predictions:\n",
        "        probs_dict = pred['probabilities']\n",
        "        winner_squad = pred['winner_squad']\n",
        "        \n",
        "        if not probs_dict:\n",
        "            continue\n",
        "        \n",
        "        # Get predicted winner and confidence\n",
        "        predicted_winner = max(probs_dict, key=probs_dict.get)\n",
        "        confidence = probs_dict[predicted_winner]\n",
        "        is_correct = 1.0 if predicted_winner == winner_squad else 0.0\n",
        "        \n",
        "        confidences.append(confidence)\n",
        "        correctness.append(is_correct)\n",
        "    \n",
        "    if len(confidences) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    confidences = np.array(confidences)\n",
        "    correctness = np.array(correctness)\n",
        "    \n",
        "    # Bin confidences and compute ECE\n",
        "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
        "    ece = 0.0\n",
        "    total_samples = len(confidences)\n",
        "    \n",
        "    for i in range(n_bins):\n",
        "        bin_lower = bin_boundaries[i]\n",
        "        bin_upper = bin_boundaries[i + 1]\n",
        "        \n",
        "        # Find samples in this bin\n",
        "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
        "        n_in_bin = in_bin.sum()\n",
        "        \n",
        "        if n_in_bin > 0:\n",
        "            # Compute average confidence and accuracy in bin\n",
        "            avg_confidence = confidences[in_bin].mean()\n",
        "            avg_accuracy = correctness[in_bin].mean()\n",
        "            \n",
        "            # Add weighted absolute difference to ECE\n",
        "            ece += (n_in_bin / total_samples) * abs(avg_accuracy - avg_confidence)\n",
        "    \n",
        "    return ece\n",
        "\n",
        "\n",
        "print(\"Metric calculation functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Top-K Accuracy Functions\n",
        "# ============================================================================\n",
        "\n",
        "def compute_topk_accuracy_all(all_predictions: List[Dict], k: int) -> Tuple[float, int]:\n",
        "    \"\"\"\n",
        "    Compute Top-K accuracy using ALL time points where exactly K teams are alive.\n",
        "    \n",
        "    Args:\n",
        "        all_predictions: List of prediction dictionaries\n",
        "        k: Number of alive teams to filter for (e.g., 4 or 8)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (accuracy, num_samples)\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for pred in all_predictions:\n",
        "        num_alive = pred['num_alive']\n",
        "        \n",
        "        # Only consider time points where exactly K teams are alive\n",
        "        if num_alive != k:\n",
        "            continue\n",
        "        \n",
        "        probs_dict = pred['probabilities']\n",
        "        winner_squad = pred['winner_squad']\n",
        "        \n",
        "        if not probs_dict:\n",
        "            continue\n",
        "        \n",
        "        # Get predicted winner\n",
        "        predicted_winner = max(probs_dict, key=probs_dict.get)\n",
        "        \n",
        "        if predicted_winner == winner_squad:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    \n",
        "    if total == 0:\n",
        "        return 0.0, 0\n",
        "    \n",
        "    return correct / total, total\n",
        "\n",
        "\n",
        "def compute_topk_accuracy_first(all_predictions: List[Dict], k: int) -> Tuple[float, int]:\n",
        "    \"\"\"\n",
        "    Compute Top-K accuracy using FIRST time point per match where exactly K teams are alive.\n",
        "    \n",
        "    Args:\n",
        "        all_predictions: List of prediction dictionaries\n",
        "        k: Number of alive teams to filter for (e.g., 4 or 8)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (accuracy, num_samples)\n",
        "    \"\"\"\n",
        "    # Group by match_id and find first occurrence\n",
        "    match_first_k = {}\n",
        "    \n",
        "    for pred in all_predictions:\n",
        "        if pred['num_alive'] != k:\n",
        "            continue\n",
        "        \n",
        "        match_id = pred['match_id']\n",
        "        time_point = pred['time_point']\n",
        "        \n",
        "        # Keep first occurrence (earliest time point) for each match\n",
        "        if match_id not in match_first_k:\n",
        "            match_first_k[match_id] = pred\n",
        "        elif time_point < match_first_k[match_id]['time_point']:\n",
        "            match_first_k[match_id] = pred\n",
        "    \n",
        "    # Compute accuracy on first occurrences only\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for pred in match_first_k.values():\n",
        "        probs_dict = pred['probabilities']\n",
        "        winner_squad = pred['winner_squad']\n",
        "        \n",
        "        if not probs_dict:\n",
        "            continue\n",
        "        \n",
        "        predicted_winner = max(probs_dict, key=probs_dict.get)\n",
        "        \n",
        "        if predicted_winner == winner_squad:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    \n",
        "    if total == 0:\n",
        "        return 0.0, 0\n",
        "    \n",
        "    return correct / total, total\n",
        "\n",
        "\n",
        "def compute_topk_accuracy_avg(all_predictions: List[Dict], k: int) -> Tuple[float, int]:\n",
        "    \"\"\"\n",
        "    Compute Top-K accuracy using AVERAGE accuracy per match (each match counts once).\n",
        "    \n",
        "    Args:\n",
        "        all_predictions: List of prediction dictionaries\n",
        "        k: Number of alive teams to filter for (e.g., 4 or 8)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (accuracy, num_samples = num_matches)\n",
        "    \"\"\"\n",
        "    # Group by match_id\n",
        "    match_predictions = defaultdict(list)\n",
        "    \n",
        "    for pred in all_predictions:\n",
        "        if pred['num_alive'] != k:\n",
        "            continue\n",
        "        \n",
        "        match_id = pred['match_id']\n",
        "        match_predictions[match_id].append(pred)\n",
        "    \n",
        "    # Compute average accuracy per match\n",
        "    match_accuracies = []\n",
        "    \n",
        "    for match_id, preds in match_predictions.items():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for pred in preds:\n",
        "            probs_dict = pred['probabilities']\n",
        "            winner_squad = pred['winner_squad']\n",
        "            \n",
        "            if not probs_dict:\n",
        "                continue\n",
        "            \n",
        "            predicted_winner = max(probs_dict, key=probs_dict.get)\n",
        "            \n",
        "            if predicted_winner == winner_squad:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "        \n",
        "        if total > 0:\n",
        "            match_accuracies.append(correct / total)\n",
        "    \n",
        "    if len(match_accuracies) == 0:\n",
        "        return 0.0, 0\n",
        "    \n",
        "    return np.mean(match_accuracies), len(match_accuracies)\n",
        "\n",
        "\n",
        "print(\"Top-K accuracy functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Phase-wise Evaluation\n",
        "# ============================================================================\n",
        "\n",
        "def compute_phase_wise_metrics(all_predictions: List[Dict]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute phase-wise metrics (Accuracy, Log-loss, ECE, Brier-score).\n",
        "    \n",
        "    Args:\n",
        "        all_predictions: List of all matched prediction dictionaries\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with metrics as rows and phases as columns\n",
        "    \"\"\"\n",
        "    # Group predictions by phase\n",
        "    phase_groups = defaultdict(list)\n",
        "    for pred in all_predictions:\n",
        "        phase = pred.get('phase')\n",
        "        if phase is not None and 1 <= phase <= NUM_PHASES:\n",
        "            phase_groups[phase].append(pred)\n",
        "    \n",
        "    # Compute metrics for each phase\n",
        "    results = {\n",
        "        'accuracy': [],\n",
        "        'log_loss': [],\n",
        "        'ece': [],\n",
        "        'brier_score': [],\n",
        "    }\n",
        "    \n",
        "    phase_labels = []\n",
        "    \n",
        "    for phase in range(1, NUM_PHASES + 1):\n",
        "        phase_labels.append(f'Phase{phase}')\n",
        "        phase_preds = phase_groups.get(phase, [])\n",
        "        \n",
        "        if len(phase_preds) == 0:\n",
        "            results['accuracy'].append(np.nan)\n",
        "            results['log_loss'].append(np.nan)\n",
        "            results['ece'].append(np.nan)\n",
        "            results['brier_score'].append(np.nan)\n",
        "            continue\n",
        "        \n",
        "        # Accuracy\n",
        "        accuracies = [\n",
        "            compute_accuracy(pred['probabilities'], pred['winner_squad'])\n",
        "            for pred in phase_preds\n",
        "        ]\n",
        "        results['accuracy'].append(np.mean(accuracies))\n",
        "        \n",
        "        # Log Loss\n",
        "        log_losses = [\n",
        "            compute_log_loss(pred['probabilities'], pred['winner_squad'])\n",
        "            for pred in phase_preds\n",
        "        ]\n",
        "        results['log_loss'].append(np.mean(log_losses))\n",
        "        \n",
        "        # ECE (computed on all phase predictions together)\n",
        "        ece = compute_ece(phase_preds)\n",
        "        results['ece'].append(ece)\n",
        "        \n",
        "        # Brier Score\n",
        "        brier_scores = [\n",
        "            compute_brier_score(pred['probabilities'], pred['winner_squad'])\n",
        "            for pred in phase_preds\n",
        "        ]\n",
        "        results['brier_score'].append(np.mean(brier_scores))\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(results, index=phase_labels).T\n",
        "    return df\n",
        "\n",
        "\n",
        "print(\"Phase-wise evaluation function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Main Evaluation Loop\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(model_name: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evaluate a single model on all prediction files.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name of the model (e.g., 'WeightedCox')\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with evaluation results\n",
        "    \"\"\"\n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "    \n",
        "    # Find all prediction JSON files for this model\n",
        "    prediction_files = sorted(glob.glob(os.path.join(PREDICTIONS_DIR, f\"*_{model_name}.json\")))\n",
        "    print(f\"  Found {len(prediction_files)} prediction files\")\n",
        "    \n",
        "    if len(prediction_files) == 0:\n",
        "        print(f\"  Warning: No prediction files found for {model_name}\")\n",
        "        return None\n",
        "    \n",
        "    all_matched_predictions = []\n",
        "    \n",
        "    # Process each prediction file\n",
        "    for pred_file in tqdm(prediction_files, desc=f\"  Processing {model_name}\"):\n",
        "        # Load prediction\n",
        "        pred_data = load_prediction_json(pred_file)\n",
        "        match_id = pred_data['match_id']\n",
        "        predictions = pred_data['predictions']\n",
        "        \n",
        "        # Extract UUID from match_id (last part after the last dot)\n",
        "        # match_id format: match.bro.custom.es_as-pgc25gs_01.steam.normal.as.2025.11.28.13.UUID\n",
        "        # We need the UUID part to match with CSV filename: pgc_2025_UUID.csv\n",
        "        uuid = match_id.split('.')[-1]\n",
        "        \n",
        "        # Find corresponding ground truth CSV using UUID\n",
        "        csv_files = glob.glob(os.path.join(DATA_PATH, f\"*{uuid}*.csv\"))\n",
        "        \n",
        "        if len(csv_files) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Load ground truth\n",
        "        ground_truth = load_ground_truth_csv(csv_files[0])\n",
        "        \n",
        "        # Match predictions with ground truth\n",
        "        matched = match_predictions_with_truth(match_id, predictions, ground_truth)\n",
        "        all_matched_predictions.extend(matched)\n",
        "    \n",
        "    print(f\"  Total matched predictions: {len(all_matched_predictions)}\")\n",
        "    \n",
        "    if len(all_matched_predictions) == 0:\n",
        "        print(f\"  Warning: No matched predictions for {model_name}\")\n",
        "        return None\n",
        "    \n",
        "    # Compute phase-wise metrics\n",
        "    phase_metrics = compute_phase_wise_metrics(all_matched_predictions)\n",
        "    \n",
        "    # Compute Top-K accuracies (3 methods)\n",
        "    # Method 1: All time points\n",
        "    top4_all_acc, top4_all_n = compute_topk_accuracy_all(all_matched_predictions, k=4)\n",
        "    top8_all_acc, top8_all_n = compute_topk_accuracy_all(all_matched_predictions, k=8)\n",
        "    \n",
        "    # Method 2: First time point per match\n",
        "    top4_first_acc, top4_first_n = compute_topk_accuracy_first(all_matched_predictions, k=4)\n",
        "    top8_first_acc, top8_first_n = compute_topk_accuracy_first(all_matched_predictions, k=8)\n",
        "    \n",
        "    # Method 3: Average per match\n",
        "    top4_avg_acc, top4_avg_n = compute_topk_accuracy_avg(all_matched_predictions, k=4)\n",
        "    top8_avg_acc, top8_avg_n = compute_topk_accuracy_avg(all_matched_predictions, k=8)\n",
        "    \n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'phase_metrics': phase_metrics,\n",
        "        'top4_all_accuracy': top4_all_acc,\n",
        "        'top4_all_samples': top4_all_n,\n",
        "        'top4_first_accuracy': top4_first_acc,\n",
        "        'top4_first_samples': top4_first_n,\n",
        "        'top4_avg_accuracy': top4_avg_acc,\n",
        "        'top4_avg_matches': top4_avg_n,\n",
        "        'top8_all_accuracy': top8_all_acc,\n",
        "        'top8_all_samples': top8_all_n,\n",
        "        'top8_first_accuracy': top8_first_acc,\n",
        "        'top8_first_samples': top8_first_n,\n",
        "        'top8_avg_accuracy': top8_avg_acc,\n",
        "        'top8_avg_matches': top8_avg_n,\n",
        "        'total_predictions': len(all_matched_predictions),\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Main evaluation function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Run Evaluation\n",
        "# ============================================================================\n",
        "# Run evaluation on all models\n",
        "results = {}\n",
        "\n",
        "for model in MODELS:\n",
        "    result = evaluate_model(model)\n",
        "    if result is not None:\n",
        "        results[model] = result\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Evaluation complete for {len(results)} models\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Display results for each model\n",
        "for model_name, result in results.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"\\nTotal predictions: {result['total_predictions']}\")\n",
        "    \n",
        "    print(f\"\\n{'-'*60}\")\n",
        "    print(\"Phase-wise Metrics:\")\n",
        "    print(f\"{'-'*60}\")\n",
        "    print(result['phase_metrics'].round(4))\n",
        "    \n",
        "    print(f\"\\n{'-'*60}\")\n",
        "    print(\"Top-K Accuracy (3 Methods):\")\n",
        "    print(f\"{'-'*60}\")\n",
        "    print(\"\\nTop-4 Accuracy:\")\n",
        "    print(f\"  All time points:  {result['top4_all_accuracy']:.4f} (n={result['top4_all_samples']} samples)\")\n",
        "    print(f\"  First per match:  {result['top4_first_accuracy']:.4f} (n={result['top4_first_samples']} matches)\")\n",
        "    print(f\"  Avg per match:    {result['top4_avg_accuracy']:.4f} (n={result['top4_avg_matches']} matches)\")\n",
        "    \n",
        "    print(\"\\nTop-8 Accuracy:\")\n",
        "    print(f\"  All time points:  {result['top8_all_accuracy']:.4f} (n={result['top8_all_samples']} samples)\")\n",
        "    print(f\"  First per match:  {result['top8_first_accuracy']:.4f} (n={result['top8_first_samples']} matches)\")\n",
        "    print(f\"  Avg per match:    {result['top8_avg_accuracy']:.4f} (n={result['top8_avg_matches']} matches)\")\n",
        "    print()\n",
        "\n",
        "# Model Comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Phase-wise Accuracy Comparison\")\n",
        "print(\"=\"*60)\n",
        "accuracy_comparison = pd.DataFrame({\n",
        "    model_name: result['phase_metrics'].loc['accuracy']\n",
        "    for model_name, result in results.items()\n",
        "})\n",
        "print(accuracy_comparison.round(4))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Phase-wise Log Loss Comparison\")\n",
        "print(\"=\"*60)\n",
        "logloss_comparison = pd.DataFrame({\n",
        "    model_name: result['phase_metrics'].loc['log_loss']\n",
        "    for model_name, result in results.items()\n",
        "})\n",
        "print(logloss_comparison.round(4))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Phase-wise ECE Comparison\")\n",
        "print(\"=\"*60)\n",
        "ece_comparison = pd.DataFrame({\n",
        "    model_name: result['phase_metrics'].loc['ece']\n",
        "    for model_name, result in results.items()\n",
        "})\n",
        "print(ece_comparison.round(4))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Phase-wise Brier Score Comparison\")\n",
        "print(\"=\"*60)\n",
        "brier_comparison = pd.DataFrame({\n",
        "    model_name: result['phase_metrics'].loc['brier_score']\n",
        "    for model_name, result in results.items()\n",
        "})\n",
        "print(brier_comparison.round(4))\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Overall Performance Summary (Averaged Across All Phases)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "summary_stats = {}\n",
        "for model_name, result in results.items():\n",
        "    phase_metrics = result['phase_metrics']\n",
        "    \n",
        "    summary_stats[model_name] = {\n",
        "        'Avg Accuracy': phase_metrics.loc['accuracy'].mean(),\n",
        "        'Avg Log Loss': phase_metrics.loc['log_loss'].mean(),\n",
        "        'Avg ECE': phase_metrics.loc['ece'].mean(),\n",
        "        'Avg Brier Score': phase_metrics.loc['brier_score'].mean(),\n",
        "        'Top-4 (First)': result['top4_first_accuracy'],\n",
        "        'Top-4 (Avg)': result['top4_avg_accuracy'],\n",
        "        'Top-8 (First)': result['top8_first_accuracy'],\n",
        "        'Top-8 (Avg)': result['top8_avg_accuracy'],\n",
        "    }\n",
        "\n",
        "summary_df = pd.DataFrame(summary_stats).T\n",
        "print(summary_df.round(4))\n",
        "\n",
        "# Save results (optional)\n",
        "SAVE_RESULTS = False  # Set to True to save\n",
        "\n",
        "if SAVE_RESULTS:\n",
        "    output_dir = \"/Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_results/pgc2025_evaluation/\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Save summary statistics\n",
        "    summary_df.to_csv(os.path.join(output_dir, \"summary_statistics_fixed.csv\"))\n",
        "    print(f\"\\nSaved summary statistics to {output_dir}/summary_statistics_fixed.csv\")\n",
        "    \n",
        "    # Save each model's phase-wise metrics\n",
        "    for model_name, result in results.items():\n",
        "        filename = f\"phase_metrics_{model_name}_fixed.csv\"\n",
        "        result['phase_metrics'].to_csv(os.path.join(output_dir, filename))\n",
        "        print(f\"Saved {model_name} phase metrics to {output_dir}/{filename}\")\n",
        "    \n",
        "    print(\"\\nAll results saved successfully!\")\n",
        "else:\n",
        "    print(\"\\nSet SAVE_RESULTS = True to save results to disk\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
