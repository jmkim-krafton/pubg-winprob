{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b47572f-7146-44a4-a86b-7d280afeede2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Batch Inference with JSON Output\n",
    "\n",
    "This notebook runs inference on multiple CSV files and saves detailed results including win probabilities and squad state information for each time point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5bb55f-37ab-4213-aa40-89487499c438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc055b8-745c-4cca-9f31-ab30b89025d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "031c6f31-4a13-4cd2-9263-127a4c8fdd8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78552ca0-3af3-4341-93db-d4e76d68b601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set the paths to your checkpoint and test CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18862ce3-108c-4c5f-ba6d-af69c205f9bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CSV_DIR = \"/Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_features/replay_single_sample_v2/\"\n",
    "CHECKPOINT_PATH = \"/Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_results/checkpoints/run22_emb1024_head2_layer6_drop0.1_lr1e-4_weighted_cox_v11/best.pt\"\n",
    "OUTPUT_DIR = \"/Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_results/predictions_weighted_cox_v2/\"\n",
    "\n",
    "# Check CUDA availability\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Find all CSV files\n",
    "csv_files = sorted(glob.glob(os.path.join(CSV_DIR, \"*.csv\")))\n",
    "\n",
    "print(f\"CSV directory: {CSV_DIR}\")\n",
    "print(f\"  Exists: {os.path.exists(CSV_DIR)}\")\n",
    "print(f\"  CSV files found: {len(csv_files)}\")\n",
    "print(f\"\\nCheckpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"  Exists: {os.path.exists(CHECKPOINT_PATH)}\")\n",
    "print(f\"\\nOutput dir: {OUTPUT_DIR}\")\n",
    "print(f\"Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4464328c-0873-4b18-a3b6-cf786a123e80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Preview CSV Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f364987e-add8-43f4-b2d5-55c8d8191467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preview CSV files\n",
    "print(\"CSV files to process:\")\n",
    "for i, csv_file in enumerate(csv_files[:10]):\n",
    "    print(f\"  {i+1}. {os.path.basename(csv_file)}\")\n",
    "\n",
    "if len(csv_files) > 10:\n",
    "    print(f\"  ... and {len(csv_files) - 10} more files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a6932fb-709b-4b2c-af8f-935682a23eb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Run Batch Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82355b12-6fc7-4304-ab9b-51c46dac5774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from src.training.inference_with_json import run_inference_and_save_json\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Run inference on all CSV files\n",
    "all_results = []\n",
    "failed_files = []\n",
    "\n",
    "for csv_file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    csv_basename = os.path.basename(csv_file)\n",
    "    \n",
    "    # Check if already processed (JSON exists)\n",
    "    json_pattern = os.path.join(OUTPUT_DIR, f\"predictions_{os.path.splitext(csv_basename)[0]}_*.json\")\n",
    "    existing_jsons = glob.glob(json_pattern)\n",
    "    \n",
    "    if existing_jsons:\n",
    "        print(f\"  Skipping {csv_basename} (already processed)\")\n",
    "        continue\n",
    "    \n",
    "    result = run_inference_and_save_json(\n",
    "        csv_path=csv_file,\n",
    "        checkpoint_path=CHECKPOINT_PATH,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        device=DEVICE,\n",
    "        temperature=None,\n",
    "    )\n",
    "    all_results.append(result)\n",
    "\n",
    "print(f\"\\nCompleted: {len(all_results)} files processed\")\n",
    "if failed_files:\n",
    "    print(f\"Failed: {len(failed_files)} files\")\n",
    "    for f in failed_files:\n",
    "        print(f\"  - {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51516e1a-bdbe-4604-afcb-55324d6da7bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List generated JSON files\n",
    "json_files = sorted(glob.glob(os.path.join(OUTPUT_DIR, \"*.json\")))\n",
    "print(f\"Total JSON files in output directory: {len(json_files)}\")\n",
    "\n",
    "for i, json_file in enumerate(json_files[:10]):\n",
    "    print(f\"  {i+1}. {os.path.basename(json_file)}\")\n",
    "\n",
    "if len(json_files) > 10:\n",
    "    print(f\"  ... and {len(json_files) - 10} more files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bd3dfa4-7155-449c-9e11-4d29f5268b8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Examine a sample JSON file\n",
    "if json_files:\n",
    "    sample_json_path = json_files[0]\n",
    "    with open(sample_json_path, 'r') as f:\n",
    "        sample_data = json.load(f)\n",
    "    \n",
    "    print(f\"Sample JSON: {os.path.basename(sample_json_path)}\")\n",
    "    print(f\"\\nMatch Info:\")\n",
    "    for key, value in sample_data['match_info'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Get first time point data\n",
    "    match_id = list(sample_data['predictions'].keys())[0]\n",
    "    time_points = sorted(sample_data['predictions'][match_id].keys(), key=float)\n",
    "    first_tp = time_points[0]\n",
    "    tp_data = sample_data['predictions'][match_id][first_tp]\n",
    "    \n",
    "    print(f\"\\nFirst Time Point ({first_tp}):\")\n",
    "    print(f\"  Phase: {tp_data['phase']}\")\n",
    "    print(f\"  Squads: {len(tp_data['squad_numbers'])}\")\n",
    "    print(f\"  Alive counts: {tp_data['alive_cnt']}\")\n",
    "    print(f\"  HP values: {tp_data['hp']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d96914e3-9ec9-4324-a689-79275980649f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show probabilities for the sample\n",
    "if json_files:\n",
    "    print(f\"\\nProbabilities at first time point:\")\n",
    "    for squad_num, prob in sorted(tp_data['probabilities'].items(), key=lambda x: -x[1]):\n",
    "        is_alive = tp_data['is_alive'].get(str(squad_num), tp_data['is_alive'].get(int(squad_num), False))\n",
    "        print(f\"  Squad {squad_num}: {prob:.4f} (alive: {is_alive})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "605ff872-5dec-49d7-bcf3-f5dd1c33ee5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Summary Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "714c05be-e7b8-434b-b3d9-1317e540cda9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Summary statistics across all JSON files\n",
    "if json_files:\n",
    "    total_matches = len(json_files)\n",
    "    maps_count = {}\n",
    "    winner_correct = 0\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        map_name = data['match_info'].get('map', 'unknown')\n",
    "        maps_count[map_name] = maps_count.get(map_name, 0) + 1\n",
    "    \n",
    "    print(f\"Total matches processed: {total_matches}\")\n",
    "    print(f\"\\nMatches by map:\")\n",
    "    for map_name, count in sorted(maps_count.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {map_name}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "568e1b6f-9552-4909-b257-72202ba2fd9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show output directory contents\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(f\"Total files: {len(os.listdir(OUTPUT_DIR))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90d84ad-9a93-433c-a231-90b30f3492d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BATCH INFERENCE COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"CSV files processed: {len(csv_files)}\")\n",
    "print(f\"JSON files generated: {len(json_files)}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "test_inference_with_json",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
