# Databricks notebook source
# MAGIC %md
# MAGIC ### PGC WWCD Prediction - Postmatch Dataset Generation
# MAGIC * Source: Esport post-match JSON files (Volume)
# MAGIC > - esport post-match: /Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_inference_match_log/
# MAGIC > - pgc 2025 post-match: /Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_2025_match_log/
# MAGIC * Destination: /Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_features/inference_v2.1/
# MAGIC
# MAGIC
# MAGIC #### Preprocessing steps
# MAGIC 1. Load post-match JSON files directly
# MAGIC 2. Parse the data using load_match_df_from_postmatch
# MAGIC 3. Extract features using SquadFeatureExtractor
# MAGIC 4. Save the extracted features as CSV

# COMMAND ----------

# MAGIC %restart_python

# COMMAND ----------

import sys, os

BASE = os.path.join(os.getcwd(), "..")
BASE = os.path.abspath(BASE)
POSTMATCH_ROOT = os.path.join(BASE, "generate_postmatch_dataset")

if BASE not in sys.path:
    sys.path.insert(0, BASE)
if POSTMATCH_ROOT not in sys.path:
    sys.path.insert(0, POSTMATCH_ROOT)

print("BASE:", BASE)
print("POSTMATCH_ROOT:", POSTMATCH_ROOT)

# COMMAND ----------

import warnings
warnings.filterwarnings("ignore", message=".*Mean of empty slice.*")
warnings.filterwarnings("ignore", message="invalid value encountered in double_scalars")

import os
import itertools
import glob
import time
import pandas as pd
import numpy as np
import concurrent.futures
import traceback
from datetime import datetime, timedelta, timezone
from pathlib import Path

from feature_engineering.variables import MAP_NAME_TO_LEGACY
from feature_engineering.parsing_match_log import MatchLogParser
from feature_engineering.extracting_pgc_features import FeatureExtractor, SquadFeatureExtractor
from load_match_df_from_postmatch import load_match_df_from_postmatch, REQUIRED_LOG_TYPES
from feature_engineering.match_table_info import target_table_and_cols

# COMMAND ----------

# PGC JSON íŒŒì¼ ê²½ë¡œ ì„¤ì •
PGC_JSON_PATH = "/Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_inference_match_log/"  # PGC JSON íŒŒì¼ ìœ„ì¹˜
DEST_PATH = "/Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_features/inference_v2.1/"

# Test match ì„¤ì •
test_match= "/Volumes/main/dl_service_dev/anticheat/jmkim/pgc/2ab28633-c919-45c5-82bb-dab37b0e8363" # PUBG RACE 10 - Match 3

# ì²˜ë¦¬ ì„¤ì •
USE_PARALLEL = True          # True: ë³‘ë ¬ ì²˜ë¦¬, False: ìˆœì°¨ ì²˜ë¦¬
MAX_WORKERS = None           # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜ ì‚¬ìš©)
START_INDEX = 0              # ì‹œì‘ íŒŒì¼ ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘)
END_INDEX = 10              # ë íŒŒì¼ ì¸ë±ìŠ¤ (Noneì´ë©´ ëê¹Œì§€, ì˜ˆ: 1000ì´ë©´ 0-999 ì²˜ë¦¬)

# ì˜ˆì‹œ:
# START_INDEX = 0, END_INDEX = 1000    -> íŒŒì¼ 0~999 ì²˜ë¦¬ (1000ê°œ)
# START_INDEX = 1000, END_INDEX = 2000 -> íŒŒì¼ 1000~1999 ì²˜ë¦¬ (1000ê°œ)
# START_INDEX = 0, END_INDEX = None    -> ëª¨ë“  íŒŒì¼ ì²˜ë¦¬

print(f"PGC JSON PATH: {PGC_JSON_PATH}")
print(f"DESTINATION PATH: {DEST_PATH}")
print(f"PROCESSING MODE: {'Parallel' if USE_PARALLEL else 'Sequential'}")
print(f"MAX WORKERS: {MAX_WORKERS}")
if END_INDEX is not None:
    print(f"FILE RANGE: {START_INDEX} to {END_INDEX-1} (total: {END_INDEX-START_INDEX} files)")
else:
    print(f"FILE RANGE: {START_INDEX} to END (all remaining files)")

# COMMAND ----------

# use_cols ìƒì„±
use_cols = list(set(list(itertools.chain(*[v for k, v in target_table_and_cols.items()]))))
required_log_types_lower = {log_type.lower() for log_type in REQUIRED_LOG_TYPES}

print(f"use_cols: {len(use_cols)}ê°œ")
print(f"REQUIRED_LOG_TYPES: {len(required_log_types_lower)}ê°œ")

# COMMAND ----------

match_df, match_id, n_events = load_match_df_from_postmatch(
            test_match, # pubg race 10 match 3
            use_log_types=required_log_types_lower,
            use_cols=use_cols,
            return_df=True
        )

# COMMAND ----------

def processing_match_log_task(path, idx, required_log_types_lower, use_cols):
    """
    Esport post-match JSON íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ squad feature datasetì„ ìƒì„±
    
    Args:
        path: Esport post-match JSON íŒŒì¼ ê²½ë¡œ
        idx: ì¸ë±ìŠ¤ (ë¡œê¹…ìš©)
        required_log_types_lower: í•„ìš”í•œ ë¡œê·¸ íƒ€ì… ëª©ë¡
        use_cols: ì‚¬ìš©í•  ì»¬ëŸ¼ ëª©ë¡
    
    Returns:
        squad_dataset: Squad feature DataFrame
    """
    try:
        task_start_time = time.time()
        print(f"[{idx}] Processing: {path}")
        
        # ==================== STEP 1: Load and Parse Match Data ====================
        step1_start = time.time()
        
        # 1. load_match_df_from_postmatchë¥¼ ì‚¬ìš©í•˜ì—¬ match_df ìƒì„±
        match_df, match_id, n_events = load_match_df_from_postmatch(
            path,
            use_log_types=required_log_types_lower,
            use_cols=use_cols,
            return_df=True
        )
        
        print(f"[{idx}] Match ID: {match_id}, Events: {n_events}, Shape: {match_df.shape}")
        
        # 2. MatchLogParser ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (match_df ì§ì ‘ ì£¼ì…)
        parser = MatchLogParser.__new__(MatchLogParser)
        parser.match_id = match_id
        parser.match_df = match_df.copy()
        parser.decorator_output = False
        
        # 3. ì „ì²˜ë¦¬
        parser.match_df = parser.normalize_ingame_time(parser.match_df)
        
        # ğŸ¯ LogMatchStart ê¸°ì¤€ character ìˆœì„œ íŒŒì‹±
        parser.character_order = parser._parse_character_order_from_match_start()
        
        # ğŸ¯ ë¸”ë£¨ì¡´/í™”ì´íŠ¸ì¡´ ì •ë³´ íŒŒì‹±
        try:
            parser.zone_info = parser.parse_zone_info(parser.match_df)
        except Exception as e:
            print(f"[{idx}] âš ï¸ zone_info ìƒì„± ì‹¤íŒ¨: {e}")
            parser.zone_info = pd.DataFrame()
        
        parser.user_match_str_len = 139 if 'competitive' in parser.match_id else 136
        
        step1_time = time.time() - step1_start
        print(f"[{idx}] â±ï¸ STEP 1 (Load & Parse): {step1_time:.3f}s")
        
        # ==================== STEP 2: Generate player_df ====================
        step2_start = time.time()
        
        # 4. player_df ìƒì„±
        player_df = parser.parse()
        
        step2_time = time.time() - step2_start
        print(f"[{idx}] â±ï¸ STEP 2 (player_df): {step2_time:.3f}s - Shape: {player_df.shape}")
        
        # ==================== STEP 3: Generate squad_dataset ====================
        step3_start = time.time()
        
        # 5. death_timeì˜ NaN ê°’ì„ 2000ìœ¼ë¡œ ì±„ìš°ê¸°
        player_df['death_time'] = player_df['death_time'].fillna(2000.0)
        
        # 5-1. NaN/inf ê°’ ì²˜ë¦¬: character_teamidì™€ character_ranking
        # character_teamidì˜ NaNì„ -1ë¡œ ëŒ€ì²´ (ìœ íš¨í•˜ì§€ ì•Šì€ íŒ€)
        player_df['character_teamid'] = player_df['character_teamid'].replace([np.inf, -np.inf], np.nan).fillna(-1)
        # character_rankingì˜ NaNì„ ìµœí•˜ìœ„ ìˆœìœ„ë¡œ ëŒ€ì²´
        max_rank = player_df['character_ranking'].replace([np.inf, -np.inf], np.nan).max()
        if pd.isna(max_rank):
            max_rank = 100  # ê¸°ë³¸ê°’
        player_df['character_ranking'] = player_df['character_ranking'].replace([np.inf, -np.inf], np.nan).fillna(max_rank)
        
        # 6. Squadë³„ ë¼ë²¨ ì •ë³´ ìƒì„±
        squad_labels = player_df.groupby('character_teamid').agg({
            'character_ranking': 'first',
            'death_time': 'max'
        }).reset_index()
        
        squad_labels.columns = ['squad_number', 'squad_ranking', 'squad_death_time']
        
        # NaN/inf ê°’ ì²˜ë¦¬ í›„ integer ë³€í™˜
        squad_labels['squad_number'] = squad_labels['squad_number'].replace([np.inf, -np.inf], np.nan).fillna(-1).astype(int)
        squad_labels['squad_ranking'] = squad_labels['squad_ranking'].replace([np.inf, -np.inf], np.nan).fillna(100).astype(int)
        squad_labels['squad_win'] = (squad_labels['squad_ranking'] == 1).astype(int)
        
        # ğŸ¯ ìš°ìŠ¹ íŒ€(ranking=1)ì€ ë¬´ì¡°ê±´ squad_death_time=2000
        squad_labels.loc[squad_labels['squad_ranking'] == 1, 'squad_death_time'] = 2000.0
        
        # 7. Squad ì •ë³´ ìƒì„± (LogMatchStart ìˆœì„œ í¬í•¨)
        squad_info = pd.DataFrame({
            'character_name': player_df['character_name'].values,
            'squad_number': player_df['character_teamid'].replace([np.inf, -np.inf], np.nan).fillna(-1).astype(int).values,
            'account_id': [idx.split('@')[0] for idx in player_df.index]  # user_matchì—ì„œ account_id ì¶”ì¶œ
        }, index=player_df.index)  # ì¸ë±ìŠ¤ ìœ ì§€
        
        # ğŸ¯ LogMatchStart ìˆœì„œ ì •ë³´ ì¶”ê°€ (ê° íŒ€ ë‚´ì—ì„œì˜ ìˆœì„œ)
        character_order = getattr(parser, 'character_order', {})
        
        def get_match_start_order(row):
            team_id = row['squad_number']
            account_id = row['account_id']
            if team_id in character_order:
                try:
                    return character_order[team_id].index(account_id)
                except ValueError:
                    return 999  # ìˆœì„œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
            return 999
        
        squad_info['match_start_order'] = squad_info.apply(get_match_start_order, axis=1)
        
        # 8. SquadFeatureExtractor ì‚¬ìš©
        squad_extractor = SquadFeatureExtractor(
            player_df, 
            squad_info, 
            zone_info=parser.zone_info
        )
        
        # 9. map ì •ë³´ ì¶”ì¶œ
        try:
            # LogMatchStartì—ì„œ mapName ì¶”ì¶œ ì‹œë„
            log_match_start = match_df[match_df['_t'] == 'logmatchstart']
            if not log_match_start.empty:
                map_name = log_match_start.iloc[0].get('mapname', 'unknown')
            
            # map_id ì„¤ì •
            if map_name != 'unknown' and map_name in MAP_NAME_TO_LEGACY:
                map_id = MAP_NAME_TO_LEGACY[map_name]
            else:
                map_id = 'unknown'
                map_name = 'unknown'
        
        except Exception as e:
            print(f"[{idx}] âš ï¸ map ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            map_id = 'unknown'
            map_name = 'unknown'
        
        # 10. Squad dataset ìƒì„±
        squad_df = squad_extractor.generate_phase_samples(
            squad_labels=squad_labels,
            match_id=match_id
        )
        
        step3_time = time.time() - step3_start
        print(f"[{idx}] â±ï¸ STEP 3 (squad_df): {step3_time:.3f}s - Samples: {len(squad_df)}")
        
        # 11. ê²½ê¸° ë‚ ì§œ ì¶”ì¶œ (LogMatchStartì—ì„œ)
        match_date = None
        try:
            log_match_start = match_df[match_df['_t'] == 'logmatchstart']
            if not log_match_start.empty:
                match_datetime = pd.to_datetime(log_match_start.iloc[0]['_d'])
                match_date = match_datetime.strftime('%Y%m%d')
        except Exception as e:
            print(f"[{idx}] âš ï¸ match_date ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            match_date = None
        
        # 12. íŒŒì¼ëª…ì—ì„œ mode ì¶”ì¶œ ë° ì¶”ê°€ ì •ë³´ í¬í•¨
        squad_df = squad_df.copy()
        
        # PGC postmatch ë°ì´í„°ëŠ” ëª¨ë‘ squad-fpp ëª¨ë“œ
        mode = 'squad-fpp'
        
        squad_df["mode"] = mode
        squad_df["map"] = map_id
        if match_date:
            squad_df["match_date"] = match_date
        
        # ==================== Summary ====================
        total_time = time.time() - task_start_time
        
        print(f"[{idx}] ğŸ“ Mode: {mode}, Map: {map_id}")
        print(f"[{idx}] â±ï¸ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print(f"[{idx}] â±ï¸ TIMING BREAKDOWN:")
        print(f"[{idx}] â±ï¸   STEP 1 (Load & Parse): {step1_time:>7.3f}s ({step1_time/total_time*100:>5.1f}%)")
        print(f"[{idx}] â±ï¸   STEP 2 (player_df):   {step2_time:>7.3f}s ({step2_time/total_time*100:>5.1f}%)")
        print(f"[{idx}] â±ï¸   STEP 3 (squad_df): {step3_time:>7.3f}s ({step3_time/total_time*100:>5.1f}%)")
        print(f"[{idx}] â±ï¸   TOTAL:                 {total_time:>7.3f}s")
        print(f"[{idx}] â±ï¸ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print(f"[{idx}] âœ… Success: {len(squad_df)} samples generated (date: {match_date})")
        
        return squad_df
    
    except Exception as e:
        print(f"[ERROR idx={idx}] {path} :: {e}")
        traceback.print_exc()
        return None

# COMMAND ----------

# MAGIC %md
# MAGIC ### Sample Test - Single File

# COMMAND ----------

# í…ŒìŠ¤íŠ¸ìš© ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
test_match= "/Volumes/main/dl_service_dev/anticheat/jmkim/pgc/2ab28633-c919-45c5-82bb-dab37b0e8363" # PUBG RACE 10 - Match 3
# /Volumes/main/dl_service_dev/anticheat/jmkim/pgc/c90469ee-da29-4d51-ae6d-631a35fbcdcf # PUBG RACE 10 - Match 4
if os.path.exists(test_match):
    print(f"ğŸ”„ Testing with: {test_match}")
    squad_df = processing_match_log_task(test_match, 0, required_log_types_lower, use_cols)
    
    if squad_df is not None:
        print(f"\nâœ… Test successful!")
        print(f"   Shape: {squad_df.shape}")
        print(f"   Columns: {list(squad_df.columns)[:10]}")
        print(f"\nğŸ“Š Sample data:")
        print(squad_df.head())
    else:
        print("âŒ Test failed!")

# COMMAND ----------

print(len(squad_df.columns))
print(squad_df.columns)

# COMMAND ----------

# í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
if squad_df is not None and len(squad_df) > 0:
    output_filename = f"pubg_race_match3_squad_dataset_251204.csv"
    output_path = os.path.join("/Volumes/main_dev/dld_ml_anticheat_test/anticheat_test_volume/pgc_wwcd/pgc_features/sample_test/", output_filename)
    
    os.makedirs(DEST_PATH, exist_ok=True)
    squad_df.to_csv(output_path, index=False)
    
    print(f"\nğŸ’¾ Test results saved to: {output_path}")
    
    # í†µê³„ ì •ë³´
    print(f"\nğŸ“Š Test Dataset Statistics:")
    print(f"   Total samples: {len(squad_df):,}")
    print(f"   Unique squads: {squad_df['squad_number'].nunique()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Parallel Processing - Multiple Files

# COMMAND ----------

def parallel_parsing(match_json_list, max_workers=32, required_log_types_lower=None, use_cols=None):
    """
    ì—¬ëŸ¬ PGC JSON íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        match_json_list: PGC JSON íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 32)
        required_log_types_lower: í•„ìš”í•œ ë¡œê·¸ íƒ€ì… ëª©ë¡
        use_cols: ì‚¬ìš©í•  ì»¬ëŸ¼ ëª©ë¡
    
    Returns:
        tuple: (final_squad_df, date_range_str) - DataFrameê³¼ ë‚ ì§œ ë²”ìœ„ ë¬¸ìì—´
    """
    st = time.time()
    all_squad_df = []

    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(processing_match_log_task, json_path, idx, required_log_types_lower, use_cols): json_path
                 for idx, json_path in enumerate(match_json_list)}
    
        for i, future in enumerate(concurrent.futures.as_completed(futures)):
            json_path = futures[future]
            if i % 10 == 0:
                print(f"ğŸ“Š {i}/{len(match_json_list)} processed ... {time.time()-st:.2f}s")
            try:
                squad_df = future.result()
                if squad_df is not None and len(squad_df) > 0:
                    all_squad_df.append(squad_df)
            except Exception as e:
                print(f"âŒ Error processing {json_path}: {e}")
                traceback.print_exc()

    if not all_squad_df:
        print("âš ï¸ No squad data parsed.")
        return None, None

    # Squad ë°ì´í„° ê²°í•©
    final_squad_df = pd.concat(all_squad_df, ignore_index=True)
    print(f"âœ… Final squad dataset shape: {final_squad_df.shape}")
    print(f"â±ï¸ Total processing time: {time.time()-st:.2f}s")
    
    # ë‚ ì§œ ë²”ìœ„ ì¶”ì¶œ
    date_range_str = None
    if 'match_date' in final_squad_df.columns:
        unique_dates = sorted(final_squad_df['match_date'].dropna().unique())
        date_range_str = unique_dates[0]
        print(f"ğŸ“… Match date range: {date_range_str}")

    return final_squad_df, date_range_str

# COMMAND ----------

# PGC JSON íŒŒì¼ ìˆ˜ì§‘
all_json_files = glob.glob(f"{PGC_JSON_PATH}**/*.json", recursive=True)
print(f"ğŸ“ Found {len(all_json_files)} total PGC JSON files")

# íŒŒì¼ ë²”ìœ„ ì„ íƒ ì ìš©
if END_INDEX is not None:
    pgc_json_files = all_json_files[START_INDEX:END_INDEX]
    print(f"ğŸ”¢ Selected files {START_INDEX} to {END_INDEX-1} ({len(pgc_json_files)} files)")
else:
    pgc_json_files = all_json_files[START_INDEX:]
    print(f"ğŸ”¢ Selected files from {START_INDEX} to end ({len(pgc_json_files)} files)")

if len(pgc_json_files) > 0:
    # íŒŒì¼ ëª©ë¡ ì¶œë ¥ (ì²˜ìŒ 10ê°œ)
    print(f"\nğŸ“‹ Selected file list (first 10):")
    for i, f in enumerate(pgc_json_files[:10]):
        actual_index = START_INDEX + i
        print(f"  [{actual_index}] {Path(f).name}")
    if len(pgc_json_files) > 10:
        print(f"  ... and {len(pgc_json_files) - 10} more files")
else:
    print(f"âš ï¸ No files selected with current range settings")

# COMMAND ----------

# ì²˜ë¦¬ ì‹¤í–‰
if len(pgc_json_files) > 0:
    os.makedirs(DEST_PATH, exist_ok=True)
    
    if USE_PARALLEL:
        # ==================== ë³‘ë ¬ ì²˜ë¦¬ ====================
        print(f"\nğŸš€ Starting PARALLEL processing for {len(pgc_json_files)} files with {MAX_WORKERS} workers...")
        
        final_squad_df, date_range_str = parallel_parsing(
            pgc_json_files, 
            max_workers=MAX_WORKERS,
            required_log_types_lower=required_log_types_lower,
            use_cols=use_cols
        )
        
        if final_squad_df is not None and len(final_squad_df) > 0:
            # ê²°ê³¼ë¥¼ match_idë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥
            print(f"\nğŸ’¾ Saving results by match_id...")
            successful_files = 0
            
            for match_id, match_df in final_squad_df.groupby('match_id'):
                # match_idì—ì„œ UUID ë¶€ë¶„ë§Œ ì¶”ì¶œ
                if '.' in match_id:
                    uuid_part = match_id.split('.')[-1]
                else:
                    uuid_part = match_id
                
                output_filename = f"pgc_squad-fpp_postmatch_{uuid_part}.csv"
                output_path = os.path.join(DEST_PATH, output_filename)
                
                match_df.to_csv(output_path, index=False)
                successful_files += 1
                print(f"   âœ… Saved: {output_filename} ({len(match_df):,} samples)")
            
            print(f"\nâœ… Parallel processing completed!")
            print(f"   Total samples: {len(final_squad_df):,}")
            print(f"   Files saved: {successful_files}")
            if date_range_str:
                print(f"   Date range: {date_range_str}")
            print(f"\nğŸ’¾ Results saved to directory: {DEST_PATH}")
        else:
            print("âŒ No data generated from parallel processing!")
    
    else:
        # ==================== ìˆœì°¨ ì²˜ë¦¬ ====================
        print(f"\nğŸš€ Starting SEQUENTIAL processing for {len(pgc_json_files)} files...")
        
        start_time = time.time()
        successful_files = 0
        failed_files = 0
        
        for idx, json_path in enumerate(pgc_json_files):
            file_start_time = time.time()
            
            try:
                print(f"\nğŸ“‚ [{idx+1}/{len(pgc_json_files)}] Processing: {Path(json_path).name}")
                
                # ê°œë³„ íŒŒì¼ ì²˜ë¦¬
                squad_dataset = processing_match_log_task(json_path, idx, required_log_types_lower, use_cols)
                
                if squad_dataset is not None and len(squad_dataset) > 0:
                    # match_id ì¶”ì¶œ
                    match_id = squad_dataset['match_id'].iloc[0] if 'match_id' in squad_dataset.columns else f"match_{idx:06d}"
                    
                    # match_idì—ì„œ UUID ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë§ˆì§€ë§‰ '.' ì´í›„)
                    if '.' in match_id:
                        uuid_part = match_id.split('.')[-1]
                    else:
                        uuid_part = match_id
                    
                    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±: pgc_squad-fpp_postmatch_{uuid}.csv
                    output_filename = f"pgc_squad-fpp_postmatch_{uuid_part}.csv"
                    
                    output_path = os.path.join(DEST_PATH, output_filename)
                    
                    # ê°œë³„ íŒŒì¼ë¡œ ì €ì¥
                    squad_dataset.to_csv(output_path, index=False)
                    
                    file_elapsed = time.time() - file_start_time
                    successful_files += 1
                    
                    print(f"   âœ… Success: {len(squad_dataset):,} samples â†’ {output_filename} ({file_elapsed:.2f}s)")
                    
                else:
                    failed_files += 1
                    print(f"   âŒ Failed: No data generated")
                    
            except Exception as e:
                failed_files += 1
                print(f"   âŒ Error: {e}")
                
            # ì§„í–‰ë¥  í‘œì‹œ
            if (idx + 1) % 10 == 0:
                elapsed = time.time() - start_time
                avg_time = elapsed / (idx + 1)
                eta = avg_time * (len(pgc_json_files) - idx - 1)
                print(f"ğŸ“Š Progress: {idx+1}/{len(pgc_json_files)} ({((idx+1)/len(pgc_json_files)*100):.1f}%) | ETA: {eta/60:.1f}min")
        
        total_elapsed = time.time() - start_time
        
        print(f"\nâœ… Sequential processing completed!")
        print(f"   Total files processed: {len(pgc_json_files)}")
        print(f"   Successful: {successful_files}")
        print(f"   Failed: {failed_files}")
        print(f"   Total time: {total_elapsed:.2f} seconds ({total_elapsed/60:.1f} minutes)")
        print(f"   Average time per file: {total_elapsed/len(pgc_json_files):.2f} seconds")
        print(f"\nğŸ’¾ Results saved to directory: {DEST_PATH}")
        print(f"ğŸ“„ File naming format: pgc_squad-fpp_postmatch_{{uuid}}.csv")

else:
    print("\nâš ï¸ No files to process!")